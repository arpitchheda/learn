{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cbfaad8-42d7-4146-b6ca-f153b1507f92"}}},{"cell_type":"markdown","source":["# CSE Coding Assignment\n## Instructions\n\n- Please answer all questions\n- You can use any language you wish (e.g. Python, Scala, SQL...)\n- Several Markdown cells require completion. Please edit the Markdown cells to include your answer.\n- Your final notebook should compile without errors when you click \"Run All\"\n\n**Please do not publish questions. This is a confidential assignment.**\n\n### Creating a Cluster\n\nYou will need to create a Databricks Cluster. More information on this process is available here: https://docs.databricks.com/user-guide/clusters/create.html"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79e6aa93-4d81-4cfb-82f4-cb24e5c5b33c"}}},{"cell_type":"markdown","source":["## Getting Started\n\n**REQUIRED:** Run the following cells exactly as written to retrieve the necessary Coding Assignment Data Sets from Amazon S3."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af916113-b24d-4b0c-8543-ccefb6094c48"}}},{"cell_type":"code","source":["%sh curl --remote-name-all 'https://files.training.databricks.com/assessments/cse-take-home/{covertype,kafka,treecover,u.data,u.item}.csv'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4622ef0-0774-40b6-a490-8464619c3f4e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n[1/5]: https://files.training.databricks.com/assessments/cse-take-home/covertype.csv --> covertype.csv\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   133  100   133    0     0   2770      0 --:--:-- --:--:-- --:--:--  2770\n\n[2/5]: https://files.training.databricks.com/assessments/cse-take-home/kafka.csv --> kafka.csv\n\n100  135k  100  135k    0     0  6459k      0 --:--:-- --:--:-- --:--:-- 6459k\n\n[3/5]: https://files.training.databricks.com/assessments/cse-take-home/treecover.csv --> treecover.csv\n\n100  642k  100  642k    0     0  17.4M      0 --:--:-- --:--:-- --:--:-- 17.4M\n\n[4/5]: https://files.training.databricks.com/assessments/cse-take-home/u.data.csv --> u.data.csv\n\n100 1932k  100 1932k    0     0  85.7M      0 --:--:-- --:--:-- --:--:-- 85.7M\n\n[5/5]: https://files.training.databricks.com/assessments/cse-take-home/u.item.csv --> u.item.csv\n\n100  230k  100  230k    0     0  28.1M      0 --:--:-- --:--:-- --:--:-- 28.1M\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/covertype.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/kafka.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/treecover.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/u.data.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/u.item.csv\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\n[1/5]: https://files.training.databricks.com/assessments/cse-take-home/covertype.csv --> covertype.csv\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   133  100   133    0     0   2770      0 --:--:-- --:--:-- --:--:--  2770\n\n[2/5]: https://files.training.databricks.com/assessments/cse-take-home/kafka.csv --> kafka.csv\n\n100  135k  100  135k    0     0  6459k      0 --:--:-- --:--:-- --:--:-- 6459k\n\n[3/5]: https://files.training.databricks.com/assessments/cse-take-home/treecover.csv --> treecover.csv\n\n100  642k  100  642k    0     0  17.4M      0 --:--:-- --:--:-- --:--:-- 17.4M\n\n[4/5]: https://files.training.databricks.com/assessments/cse-take-home/u.data.csv --> u.data.csv\n\n100 1932k  100 1932k    0     0  85.7M      0 --:--:-- --:--:-- --:--:-- 85.7M\n\n[5/5]: https://files.training.databricks.com/assessments/cse-take-home/u.item.csv --> u.item.csv\n\n100  230k  100  230k    0     0  28.1M      0 --:--:-- --:--:-- --:--:-- 28.1M\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/covertype.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/kafka.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/treecover.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/u.data.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/u.item.csv\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/databricks/driver/covertype.csv\", \"dbfs:/FileStore/tmp/covertype.csv\")\ndbutils.fs.cp(\"file:/databricks/driver/kafka.csv\", \"dbfs:/FileStore/tmp/kafka.csv\")\ndbutils.fs.cp(\"file:/databricks/driver/treecover.csv\", \"dbfs:/FileStore/tmp/treecover.csv\")\ndbutils.fs.cp(\"file:/databricks/driver/u.data.csv\", \"dbfs:/FileStore/tmp/u.data.csv\")\ndbutils.fs.cp(\"file:/databricks/driver/u.item.csv\", \"dbfs:/FileStore/tmp/u.item.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e050ea2d-e539-4efa-aa18-813102bbe3e5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[2]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[2]: True"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Part 1: Reading and Parsing Data\n\n### Question 1:  Code Challenge - Load a CSV\n\n- Load the CSV file at `dbfs:/FileStore/tmp/nl/treecover.csv` into a DataFrame.\n- Use Apache Spark to read in the data, assigned to the variable `treeCoverDF`.\n- Your method to get the CSV file into Databricks isn't graded. We are only concerned with how you use Spark to parse and load the actual data. \n- Please use the `inferSchema` option."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6017a5eb-8c81-460e-902d-2b69588a7bd5"}}},{"cell_type":"code","source":["# YOUR CODE HERE\n\ntreeCoverDF = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tmp/treecover.csv')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58e3f180-5eed-4f10-838f-f96408365dcc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 2:  Code Challenge - Print the Schema\n\nUse Apache Spark to display the Schema of the `treeCoverDF` Dataframe."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7507f053-7a27-4422-aa1a-86dd75037f1b"}}},{"cell_type":"code","source":["# YOUR CODE HERE\ntreeCoverDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d817e77-2d16-4f02-80be-e80af133ca82"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- Id: integer (nullable = true)\n |-- Elevation: integer (nullable = true)\n |-- Aspect: integer (nullable = true)\n |-- Slope: integer (nullable = true)\n |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n |-- Cover_Type: integer (nullable = true)\n |-- Soil_Type: integer (nullable = true)\n |-- Wilderness_Area: integer (nullable = true)\n |-- Hillshade: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- Id: integer (nullable = true)\n |-- Elevation: integer (nullable = true)\n |-- Aspect: integer (nullable = true)\n |-- Slope: integer (nullable = true)\n |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n |-- Cover_Type: integer (nullable = true)\n |-- Soil_Type: integer (nullable = true)\n |-- Wilderness_Area: integer (nullable = true)\n |-- Hillshade: string (nullable = true)\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Question 3:  Code Challenge - Rows & Columns\n\nUse Apache Spark to display the number of rows and columns in the DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e44ffca2-82f1-43ea-b873-912674165b67"}}},{"cell_type":"code","source":["# YOUR CODE HERE\nprint('No of Rows ', treeCoverDF.count())\n\ncolumn_list = ','.join(treeCoverDF.columns)\nprint('Column Names : ', column_list)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84ca332b-64b2-4608-98ad-95dc7a0925a1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"No of Rows  15120\nColumn Names :  Id,Elevation,Aspect,Slope,Horizontal_Distance_To_Hydrology,Vertical_Distance_To_Hydrology,Horizontal_Distance_To_Roadways,Horizontal_Distance_To_Fire_Points,Cover_Type,Soil_Type,Wilderness_Area,Hillshade\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["No of Rows  15120\nColumn Names :  Id,Elevation,Aspect,Slope,Horizontal_Distance_To_Hydrology,Vertical_Distance_To_Hydrology,Horizontal_Distance_To_Roadways,Horizontal_Distance_To_Fire_Points,Cover_Type,Soil_Type,Wilderness_Area,Hillshade\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#Part 2: Analysis"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14c4e7ed-31f5-4ec2-b8a0-95b505985c35"}}},{"cell_type":"markdown","source":["### Question 4:  Code Challenge - Summary Statistics for a Feature\n\nUse Apache Spark to answer these questions about the `treeCoverDF` DataFrame:\n- What is the range - minimum and maximum - of values for the feature `elevation`?\n- What are the mean and standard deviation of the feature `elevation`?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cecb0ee4-834c-484c-9a8d-a1763cb22109"}}},{"cell_type":"code","source":["# YOUR CODE HERE\n\nfrom pyspark.sql import functions as f\n\n\n\nprint('Elevation Stats : ', treeCoverDF.agg(f.min(f.col('Elevation')).alias('Min_Elevation'),f.max(f.col('Elevation')).alias('Max_Elevation'),f.avg(f.col('Elevation')).alias('Avg_Elevation'),f.stddev(f.col('Elevation')).alias('StdDev_Elevation')).show())\n#print('Max Elevation : ', treeCoverDF.agg(max('Elevation')).show())\n#print('Mean Elevation : ', treeCoverDF.agg(mean('Elevation')).show())\n#print('Std Deviation Elevation : ', treeCoverDF.agg(std('Elevation')).show())\n\n\ntreeCoverDF.select(['Elevation']).describe().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68a58613-9c3c-46fe-8a43-7cbcc8df2875"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+-------------+------------------+------------------+\n|Min_Elevation|Max_Elevation|     Avg_Elevation|  StdDev_Elevation|\n+-------------+-------------+------------------+------------------+\n|         1863|         3849|2749.3225529100528|417.67818734804985|\n+-------------+-------------+------------------+------------------+\n\nElevation Stats :  None\n+-------+------------------+\n|summary|         Elevation|\n+-------+------------------+\n|  count|             15120|\n|   mean|2749.3225529100528|\n| stddev|417.67818734804985|\n|    min|              1863|\n|    max|              3849|\n+-------+------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+-------------+------------------+------------------+\n|Min_Elevation|Max_Elevation|     Avg_Elevation|  StdDev_Elevation|\n+-------------+-------------+------------------+------------------+\n|         1863|         3849|2749.3225529100528|417.67818734804985|\n+-------------+-------------+------------------+------------------+\n\nElevation Stats :  None\n+-------+------------------+\n|summary|         Elevation|\n+-------+------------------+\n|  count|             15120|\n|   mean|2749.3225529100528|\n| stddev|417.67818734804985|\n|    min|              1863|\n|    max|              3849|\n+-------+------------------+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Answer #4:\n\n- Min `elevation`: `1863`\n- Max `elevation`: `3849`\n- Mean `elevation`: `2749.32`\n- Standard Deviation of `elevation`: `417.67`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eacc2888-9f7d-4d36-9e08-d0eac0808c4e"}}},{"cell_type":"markdown","source":["### Question 5:  Code Challenge - Record Count\n\nUse Apache Spark to answer the following question:\n- How many entries in the dataset have an `elevation` greater than or equal to 2749.32 meters **AND** a `Cover_Type` of 1 or 2?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e94fccb6-891e-4092-9d46-8b32a9265b16"}}},{"cell_type":"code","source":["# YOUR CODE HERE\n## & treeCoverDF['Cover_Type'].isin([1,2])\ntreeCoverDF.filter( (treeCoverDF['Elevation'] >= 2749.32) & (treeCoverDF['Cover_Type'].isin([1,2])) ).count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ac85423-80db-45d8-b580-609d3a6ef929"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[7]: 3883","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[7]: 3883"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Question 6: Code Challenge - Compute a Percentage\n\nUse Apache Spark to answer the following question:\n- What percentage of entries with `Cover_Type` 1 or 2 have an `elevation` at or above 2749.32 meters?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51ff1431-1305-46fc-a59a-7b2378770d4a"}}},{"cell_type":"code","source":["# YOUR CODE HERE\n\ncnt = treeCoverDF.filter( (treeCoverDF['Elevation'] >= 2749.32) & (treeCoverDF['Cover_Type'].isin([1,2])) ).count()\ntotal_cnt = treeCoverDF.count()\n\nprint('% of Entries {:.6}'.format((cnt/total_cnt) * 100 ))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ec8a16b-d4e7-4498-85cf-abe310618b2d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"% of Entries 25.6812\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["% of Entries 25.6812\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Question 7: Code Challenge - Visualize Feature Distribution\n\nUse any [visualization tool available in the Databricks Runtime](https://docs.databricks.com/user-guide/visualizations/index.html) to generate the following visualization:\n\n- a bar chart that helps visualize the distribution of different Wilderness Areas in our dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ed1a9f4-703b-4332-b31f-bcea9ad80b54"}}},{"cell_type":"code","source":["# YOUR CODE HERE\n\n\n#treeCoverDF.select('Wilderness_Area').show()\n\ndisplay(treeCoverDF.select('Wilderness_Area').groupBy('Wilderness_Area').count().orderBy('Wilderness_Area'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77a28563-31ab-4e56-8730-4d9a1ede84d5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,3597],[2,499],[3,6349],[4,4675]],"plotOptions":{"displayType":"plotlyBar","customPlotOptions":{"plotlyBar":[{"key":"grouped","value":true},{"key":"stacked","value":false},{"key":"100_stacked","value":false}],"histogram":[{"key":"bins","value":"20"}]},"pivotColumns":["Wilderness_Area"],"pivotAggregation":"sum","xColumns":["Wilderness_Area"],"yColumns":["count"]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Wilderness_Area","type":"\"integer\"","metadata":"{}"},{"name":"count","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Wilderness_Area</th><th>count</th></tr></thead><tbody><tr><td>1</td><td>3597</td></tr><tr><td>2</td><td>499</td></tr><tr><td>3</td><td>6349</td></tr><tr><td>4</td><td>4675</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Question 8: Code Challenge - Visualize Average Elevation by Cover Type \n\nUse any [visualization tool available in the Databricks Runtime](https://docs.databricks.com/user-guide/visualizations/index.html) to generate the following visualization:\n\n- a bar chart showing the average elevation of each cover type with string labels for cover type\n\n**NOTE: you will need to match the integer values in the column `treeCoverDF.Cover_Type` to the string values in `dbfs:/FileStore/tmp/nl/covertype.csv` to retrieve the Cover Type Labels. It is recommended to use an Apache Spark join.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"821447ca-4ff7-448a-9cad-10691d55bef6"}}},{"cell_type":"code","source":["# YOUR CODE HERE\n\ntreeCoverTypeDF = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tmp/covertype.csv')\n\ntreeCoverTypeDF.printSchema()\n\ntreeCoverDFJoin=treeCoverDF.join(treeCoverTypeDF,treeCoverDF.Cover_Type ==  treeCoverTypeDF.cover_type_key,\"inner\")\n\ndisplay(treeCoverDFJoin.groupBy('cover_type_label').avg('Elevation').orderBy('cover_type_label'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"992c86ce-502a-4f58-967f-023813bfbd9c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- cover_type_key: integer (nullable = true)\n |-- cover_type_label: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- cover_type_key: integer (nullable = true)\n |-- cover_type_label: string (nullable = true)\n\n"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Aspen",2786.801388888889],["Cottonwood/Willow",2223.42037037037],["Douglas-fir",2423.276851851852],["Krummholz",3362.7699074074076],["Lodgepole Pine",2922.5402777777776],["Ponderosa Pine",2398.4231481481484],["Spruce/Fir",3128.025925925926]],"plotOptions":{"displayType":"plotlyBar","customPlotOptions":{"plotlyBar":[{"key":"grouped","value":true},{"key":"stacked","value":false},{"key":"100_stacked","value":false}]},"pivotColumns":[],"pivotAggregation":"sum","xColumns":["cover_type_label"],"yColumns":["avg(Elevation)"]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"cover_type_label","type":"\"string\"","metadata":"{}"},{"name":"avg(Elevation)","type":"\"double\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>cover_type_label</th><th>avg(Elevation)</th></tr></thead><tbody><tr><td>Aspen</td><td>2786.801388888889</td></tr><tr><td>Cottonwood/Willow</td><td>2223.42037037037</td></tr><tr><td>Douglas-fir</td><td>2423.276851851852</td></tr><tr><td>Krummholz</td><td>3362.7699074074076</td></tr><tr><td>Lodgepole Pine</td><td>2922.5402777777776</td></tr><tr><td>Ponderosa Pine</td><td>2398.4231481481484</td></tr><tr><td>Spruce/Fir</td><td>3128.025925925926</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#Part 3: Data Ingestion, Cleansing, and Transformations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c9c08c2-633c-48f4-88ce-64fef0946c02"}}},{"cell_type":"markdown","source":["## Instructions \n\nThis is a multi-step, data pipeline question in which you need to achieve a few objectives to build a successful job.\n\n### Data Sets\n\n#### `u.data.csv`\n\n- The full u data set, 100000 ratings by 943 users on 1682 items. \n- Each user has rated at least 20 movies.  \n- Users and items are numbered consecutively from 1. \n- The data is randomly ordered. \n- This is a tab separated file consisting of four columns: \n   - user id \n   - movie id \n   - rating \n   - date (unix seconds since 1/1/1970 UTC)\n\n#### Desired schema\n\n- `user_id INTEGER`\n- `movie_id INTEGER`\n- `rating INTEGER`\n- `date DATE `\n\n#### `u.item.csv`\n\n- This is a `|` separated file consisting of six columns:\n   - movie id\n   - movie title\n   - release date\n   - video release date\n   - IMDb URL\n   - genre\n- movie ids in this file match movie ids in `u.data`.\n\n#### Desired schema\n\n- `movie_id INTEGER`\n- `movie_title STRING`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7324d228-43d3-4eb8-8624-35fe5a11be26"}}},{"cell_type":"markdown","source":["### Question 9:  Code Challenge - Load DataFrames\n\nUse Apache Spark to perform the following:\n1. define the correct schemas for each Data Set to be imported as described above  \n   **note:** \n      - for `u.data.csv`, `date` *must* be stored using `DateType` with the format `yyyy-MM-dd`\n      - you may need to ingest `timestamp` data using `IntegerType`\n      - be sure to drop unneccesary columns for `u.item.csv`\n1. import the two files as DataFrames names `uDataDF` and `uItemDF` using the schemas you defined and these paths:\n   - `dbfs:/FileStore/tmp/u.data.csv`\n   - `dbfs:/FileStore/tmp/u.item.csv`\n1. order the `uDataDF` DataFrame by the `date` column\n\n**NOTE:** Please display the DataFrames, `uDataDF` and `uItemDF` after loading."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83b2d516-a0ea-4665-a654-fb2da2e26b18"}}},{"cell_type":"markdown","source":["#### `uDataDF`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a7e6125-08ad-4431-85bc-63a94c70e3e0"}}},{"cell_type":"code","source":["# YOUR CODE HERE\n## https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/\n\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType,LongType,TimestampType,DateType\nuDataDF_schema = StructType([StructField(\"user_id\",IntegerType(),True),StructField(\"movie_id\",IntegerType(),True),StructField(\"rating\",IntegerType(),True),StructField(\"date\",LongType(),True)])\n\nuDataDF = spark.read.format('csv').options(header=True, delimiter='\\t').schema(uDataDF_schema).load('/FileStore/tmp/u.data.csv')\nuDataDF = uDataDF.withColumn(\"date\", uDataDF[\"date\"].cast(TimestampType()).cast(DateType()))\n\nuDataDF.printSchema()\n\nprint(\"No of rows \" , uDataDF.count())\n\nuDataDF.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb26c967-2f8f-4f44-88e4-889760214a11"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- user_id: integer (nullable = true)\n |-- movie_id: integer (nullable = true)\n |-- rating: integer (nullable = true)\n |-- date: date (nullable = true)\n\nNo of rows  99999\n+-------+--------+------+----------+\n|user_id|movie_id|rating|      date|\n+-------+--------+------+----------+\n|    186|     302|     3|1998-04-04|\n|     22|     377|     1|1997-11-07|\n|    244|      51|     2|1997-11-27|\n|    166|     346|     1|1998-02-02|\n|    298|     474|     4|1998-01-07|\n|    115|     265|     2|1997-12-03|\n|    253|     465|     5|1998-04-03|\n|    305|     451|     3|1998-02-01|\n|      6|      86|     3|1997-12-31|\n|     62|     257|     2|1997-11-12|\n+-------+--------+------+----------+\nonly showing top 10 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- user_id: integer (nullable = true)\n |-- movie_id: integer (nullable = true)\n |-- rating: integer (nullable = true)\n |-- date: date (nullable = true)\n\nNo of rows  99999\n+-------+--------+------+----------+\n|user_id|movie_id|rating|      date|\n+-------+--------+------+----------+\n|    186|     302|     3|1998-04-04|\n|     22|     377|     1|1997-11-07|\n|    244|      51|     2|1997-11-27|\n|    166|     346|     1|1998-02-02|\n|    298|     474|     4|1998-01-07|\n|    115|     265|     2|1997-12-03|\n|    253|     465|     5|1998-04-03|\n|    305|     451|     3|1998-02-01|\n|      6|      86|     3|1997-12-31|\n|     62|     257|     2|1997-11-12|\n+-------+--------+------+----------+\nonly showing top 10 rows\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#### `uItemDF`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46675f81-dc38-4b36-8621-bb5a1956c54b"}}},{"cell_type":"code","source":["# YOUR CODE HERE\n## https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType,LongType,TimestampType,DateType\nuItemDF_schema = StructType([StructField(\"movie_id\",IntegerType(),True),StructField(\"movie_title\",StringType(),True),StructField(\"release_date\",LongType(),True),StructField(\"video_release_date\",LongType(),True),StructField(\"imbd_url\",StringType(),True),StructField(\"genre\",StringType(),True)])\n\nuItemDF = spark.read.format('csv').options(header=True, delimiter='|').schema(uItemDF_schema).load('/FileStore/tmp/u.item.csv')\nuItemDF = uItemDF.withColumn(\"release_date\", uItemDF[\"release_date\"].cast(TimestampType()).cast(DateType()))\nuItemDF = uItemDF.withColumn(\"video_release_date\", uItemDF[\"video_release_date\"].cast(TimestampType()).cast(DateType()))\n\nuItemDF.printSchema()\n\nprint(\"No of rows \" , uItemDF.count())\n\nuItemDF.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"829d780b-9c33-4bfa-a46e-a4538b4cc5c9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- movie_id: integer (nullable = true)\n |-- movie_title: string (nullable = true)\n |-- release_date: date (nullable = true)\n |-- video_release_date: date (nullable = true)\n |-- imbd_url: string (nullable = true)\n |-- genre: string (nullable = true)\n\nNo of rows  1681\n+--------+--------------------+------------+------------------+--------------------+-----+\n|movie_id|         movie_title|release_date|video_release_date|            imbd_url|genre|\n+--------+--------------------+------------+------------------+--------------------+-----+\n|       2|    GoldenEye (1995)|        null|              null|http://us.imdb.co...|    0|\n|       3|   Four Rooms (1995)|        null|              null|http://us.imdb.co...|    0|\n|       4|   Get Shorty (1995)|        null|              null|http://us.imdb.co...|    0|\n|       5|      Copycat (1995)|        null|              null|http://us.imdb.co...|    0|\n|       6|Shanghai Triad (Y...|        null|              null|http://us.imdb.co...|    0|\n|       7|Twelve Monkeys (1...|        null|              null|http://us.imdb.co...|    0|\n|       8|         Babe (1995)|        null|              null|http://us.imdb.co...|    0|\n|       9|Dead Man Walking ...|        null|              null|http://us.imdb.co...|    0|\n|      10|  Richard III (1995)|        null|              null|http://us.imdb.co...|    0|\n|      11|Seven (Se7en) (1995)|        null|              null|http://us.imdb.co...|    0|\n+--------+--------------------+------------+------------------+--------------------+-----+\nonly showing top 10 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- movie_id: integer (nullable = true)\n |-- movie_title: string (nullable = true)\n |-- release_date: date (nullable = true)\n |-- video_release_date: date (nullable = true)\n |-- imbd_url: string (nullable = true)\n |-- genre: string (nullable = true)\n\nNo of rows  1681\n+--------+--------------------+------------+------------------+--------------------+-----+\n|movie_id|         movie_title|release_date|video_release_date|            imbd_url|genre|\n+--------+--------------------+------------+------------------+--------------------+-----+\n|       2|    GoldenEye (1995)|        null|              null|http://us.imdb.co...|    0|\n|       3|   Four Rooms (1995)|        null|              null|http://us.imdb.co...|    0|\n|       4|   Get Shorty (1995)|        null|              null|http://us.imdb.co...|    0|\n|       5|      Copycat (1995)|        null|              null|http://us.imdb.co...|    0|\n|       6|Shanghai Triad (Y...|        null|              null|http://us.imdb.co...|    0|\n|       7|Twelve Monkeys (1...|        null|              null|http://us.imdb.co...|    0|\n|       8|         Babe (1995)|        null|              null|http://us.imdb.co...|    0|\n|       9|Dead Man Walking ...|        null|              null|http://us.imdb.co...|    0|\n|      10|  Richard III (1995)|        null|              null|http://us.imdb.co...|    0|\n|      11|Seven (Se7en) (1995)|        null|              null|http://us.imdb.co...|    0|\n+--------+--------------------+------------+------------------+--------------------+-----+\nonly showing top 10 rows\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Question 10:  Code Challenge - Perform a Join\n\nUse Apache Spark to do the following:\n- join `uDataDF` and `uItemDf` on `movie_id` as a new DataFrame called `uMovieDF`  \n   **note:** make sure you do not create duplicate `movie_id` columns\n   \n**NOTE:** Please display the DataFrame `uMovieDF`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"686d417e-2180-43a7-8785-66abb3a9fcf9"}}},{"cell_type":"code","source":["# YOUR CODE HERE\n\nuMovieDF = uDataDF.join(uItemDF,on=['movie_id'], how=\"inner\").select(uDataDF[\"*\"],uItemDF[\"movie_title\"],uItemDF[\"release_date\"],uItemDF[\"video_release_date\"],uItemDF[\"imbd_url\"],uItemDF[\"genre\"])\n\nuMovieDF.printSchema()\n\nuMovieDF.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7c405db-dd36-46ca-9ebc-6e811d53c3ca"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- user_id: integer (nullable = true)\n |-- movie_id: integer (nullable = true)\n |-- rating: integer (nullable = true)\n |-- date: date (nullable = true)\n |-- movie_title: string (nullable = true)\n |-- release_date: date (nullable = true)\n |-- video_release_date: date (nullable = true)\n |-- imbd_url: string (nullable = true)\n |-- genre: string (nullable = true)\n\n+-------+--------+------+----------+--------------------+------------+------------------+--------------------+-----+\n|user_id|movie_id|rating|      date|         movie_title|release_date|video_release_date|            imbd_url|genre|\n+-------+--------+------+----------+--------------------+------------+------------------+--------------------+-----+\n|    186|     302|     3|1998-04-04|L.A. Confidential...|        null|              null|http://us.imdb.co...|    0|\n|     22|     377|     1|1997-11-07| Heavyweights (1994)|        null|              null|http://us.imdb.co...|    0|\n|    244|      51|     2|1997-11-27|Legends of the Fa...|        null|              null|http://us.imdb.co...|    0|\n|    166|     346|     1|1998-02-02| Jackie Brown (1997)|        null|              null|http://us.imdb.co...|    0|\n|    298|     474|     4|1998-01-07|Dr. Strangelove o...|        null|              null|http://us.imdb.co...|    0|\n|    115|     265|     2|1997-12-03|Hunt for Red Octo...|        null|              null|http://us.imdb.co...|    0|\n|    253|     465|     5|1998-04-03|Jungle Book, The ...|        null|              null|http://us.imdb.co...|    0|\n|    305|     451|     3|1998-02-01|       Grease (1978)|        null|              null|http://us.imdb.co...|    0|\n|      6|      86|     3|1997-12-31|Remains of the Da...|        null|              null|http://us.imdb.co...|    0|\n|     62|     257|     2|1997-11-12| Men in Black (1997)|        null|              null|http://us.imdb.co...|    0|\n+-------+--------+------+----------+--------------------+------------+------------------+--------------------+-----+\nonly showing top 10 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- user_id: integer (nullable = true)\n |-- movie_id: integer (nullable = true)\n |-- rating: integer (nullable = true)\n |-- date: date (nullable = true)\n |-- movie_title: string (nullable = true)\n |-- release_date: date (nullable = true)\n |-- video_release_date: date (nullable = true)\n |-- imbd_url: string (nullable = true)\n |-- genre: string (nullable = true)\n\n+-------+--------+------+----------+--------------------+------------+------------------+--------------------+-----+\n|user_id|movie_id|rating|      date|         movie_title|release_date|video_release_date|            imbd_url|genre|\n+-------+--------+------+----------+--------------------+------------+------------------+--------------------+-----+\n|    186|     302|     3|1998-04-04|L.A. Confidential...|        null|              null|http://us.imdb.co...|    0|\n|     22|     377|     1|1997-11-07| Heavyweights (1994)|        null|              null|http://us.imdb.co...|    0|\n|    244|      51|     2|1997-11-27|Legends of the Fa...|        null|              null|http://us.imdb.co...|    0|\n|    166|     346|     1|1998-02-02| Jackie Brown (1997)|        null|              null|http://us.imdb.co...|    0|\n|    298|     474|     4|1998-01-07|Dr. Strangelove o...|        null|              null|http://us.imdb.co...|    0|\n|    115|     265|     2|1997-12-03|Hunt for Red Octo...|        null|              null|http://us.imdb.co...|    0|\n|    253|     465|     5|1998-04-03|Jungle Book, The ...|        null|              null|http://us.imdb.co...|    0|\n|    305|     451|     3|1998-02-01|       Grease (1978)|        null|              null|http://us.imdb.co...|    0|\n|      6|      86|     3|1997-12-31|Remains of the Da...|        null|              null|http://us.imdb.co...|    0|\n|     62|     257|     2|1997-11-12| Men in Black (1997)|        null|              null|http://us.imdb.co...|    0|\n+-------+--------+------+----------+--------------------+------------+------------------+--------------------+-----+\nonly showing top 10 rows\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Question 11:  Code Challenge - Perform an Aggregation\n\nUse Apache Spark to do the following:\n1. create an aggregate DataFrame, `aggDF` by\n  1. extracting the year from the `date` (of the review)\n  1. getting the average rating of each film per year as a column named `average_rating`\n  1. ordering descending by year and average rating\n1. write the resulting dataframe to a table named \"movie_by_year_average_rating\" in the Default database  \n   **note:** use `mode(overwrite)` \n\n#### Desired Schema\nThe schema of you resulting DataFrame should be:\n- `year INTEGER`\n- `movie_title STRING`\n- `average_rating DOUBLE`\n\n**NOTE:** Please display the DataFrame `aggDF`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f033344-61ea-4ea7-a51d-ebaf22be8e62"}}},{"cell_type":"code","source":["# YOUR CODE HERE\n\nfrom pyspark.sql.functions import year\n\naggDF = uMovieDF.select(year(\"date\").alias('year'),uMovieDF['movie_title'],uMovieDF['rating']).groupBy(['year','movie_title']).agg(f.avg(f.col('rating')).alias('average_rating')).orderBy(['year','average_rating'],ascending=False)\n\naggDF.printSchema()\n\nprint(aggDF.show(10))\n\naggDF.write.mode(\"overwrite\").saveAsTable(\"movie_by_year_average_rating\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"565e6948-92f1-4c25-a215-78c14faa8c8e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- year: integer (nullable = true)\n |-- movie_title: string (nullable = true)\n |-- average_rating: double (nullable = true)\n\n+----+--------------------+--------------+\n|year|         movie_title|average_rating|\n+----+--------------------+--------------+\n|1998|Marlene Dietrich:...|           5.0|\n|1998|        Faust (1994)|           5.0|\n|1998|Mina Tannenbaum (...|           5.0|\n|1998|Great Day in Harl...|           5.0|\n|1998|Entertaining Ange...|           5.0|\n|1998|     Star Kid (1997)|           5.0|\n|1998|The Deadly Cure (...|           5.0|\n|1998|  Prefontaine (1997)|           5.0|\n|1998|Visitors, The (Vi...|           5.0|\n|1998|Year of the Horse...|          4.75|\n+----+--------------------+--------------+\nonly showing top 10 rows\n\nNone\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- year: integer (nullable = true)\n |-- movie_title: string (nullable = true)\n |-- average_rating: double (nullable = true)\n\n+----+--------------------+--------------+\n|year|         movie_title|average_rating|\n+----+--------------------+--------------+\n|1998|Marlene Dietrich:...|           5.0|\n|1998|        Faust (1994)|           5.0|\n|1998|Mina Tannenbaum (...|           5.0|\n|1998|Great Day in Harl...|           5.0|\n|1998|Entertaining Ange...|           5.0|\n|1998|     Star Kid (1997)|           5.0|\n|1998|The Deadly Cure (...|           5.0|\n|1998|  Prefontaine (1997)|           5.0|\n|1998|Visitors, The (Vi...|           5.0|\n|1998|Year of the Horse...|          4.75|\n+----+--------------------+--------------+\nonly showing top 10 rows\n\nNone\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Part 4: Fun with JSON\n\nJSON values are typically passed by message brokers such as Kafka or Kinesis in a string encoding. When consumed by a Spark Structured Streaming application, this json must be converted into a nested object in order to be used.\n\nBelow is a list of json strings that represents how data might be passed from a message broker.\n\n**Note:** Make sure to run the cell below to retrieve the sample data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0c84485-ab71-4cb8-85bc-244494d2cb4d"}}},{"cell_type":"code","source":["%python\n\n\nsampleJson = [\n ('{\"user\":100, \"ips\" : [\"191.168.192.101\", \"191.168.192.103\", \"191.168.192.96\", \"191.168.192.99\"]}',), \n ('{\"user\":101, \"ips\" : [\"191.168.192.102\", \"191.168.192.105\", \"191.168.192.103\", \"191.168.192.107\"]}',), \n ('{\"user\":102, \"ips\" : [\"191.168.192.105\", \"191.168.192.101\", \"191.168.192.105\", \"191.168.192.107\"]}',), \n ('{\"user\":103, \"ips\" : [\"191.168.192.96\", \"191.168.192.100\", \"191.168.192.107\", \"191.168.192.101\"]}',), \n ('{\"user\":104, \"ips\" : [\"191.168.192.99\", \"191.168.192.99\", \"191.168.192.102\", \"191.168.192.99\"]}',), \n ('{\"user\":105, \"ips\" : [\"191.168.192.99\", \"191.168.192.99\", \"191.168.192.100\", \"191.168.192.96\"]}',), \n]\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba6bf8b7-4348-4b81-bc5f-d3b3c940db55"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 12:  Code Challenge - Count the IPs\n\nUse any coding techniques known to you to parse this list of JSON strings to answer the following question:\n- how many occurrences of each IP address are in this list?\n\n#### Desired Output\nYour results should be this:\n\n\n| ip | count |\n|:-:|:-:|\n| `191.168.192.96` | `3` |\n| `191.168.192.99` | `6` |\n| `191.168.192.100` | `2` |\n| `191.168.192.101` | `3` |\n| `191.168.192.102` | `2` |\n| `191.168.192.103` | `2` |\n| `191.168.192.105` | `3` |\n| `191.168.192.107` | `3` |\n\n**NOTE:** The order of your results is not important."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e34335a-ad2d-4ace-b01c-6144a67afad2"}}},{"cell_type":"code","source":["# YOUR CODE HERE\nfrom pyspark.sql.functions import explode\n## https://kontext.tech/column/spark/284/pyspark-convert-json-string-column-to-array-of-object-structtype-in-data-frame\nsampleJSON2 = []\nimport ast\nfor item in sampleJson:\n  item_dict = ast.literal_eval(item[0])\n  data = [item_dict]\n  sampleJSON2.append(data)\n\n  \nuipsDF = sqlContext.read.json(sc.parallelize(sampleJSON2))  \n#uipsDF.show()\n#uipsDF.printSchema()\nuipDF = uipsDF.select(explode('ips').alias('ip'))\n#uipDF.show()\n\nuipAggDF = uipDF.select('ip').groupBy('ip').count()\n\nuipAggDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"356e7fd8-d527-4e25-ac9f-2e006942b048"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------------+-----+\n|             ip|count|\n+---------------+-----+\n| 191.168.192.96|    3|\n|191.168.192.101|    3|\n|191.168.192.103|    2|\n| 191.168.192.99|    6|\n|191.168.192.105|    3|\n|191.168.192.107|    3|\n|191.168.192.102|    2|\n|191.168.192.100|    2|\n+---------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------------+-----+\n|             ip|count|\n+---------------+-----+\n| 191.168.192.96|    3|\n|191.168.192.101|    3|\n|191.168.192.103|    2|\n| 191.168.192.99|    6|\n|191.168.192.105|    3|\n|191.168.192.107|    3|\n|191.168.192.102|    2|\n|191.168.192.100|    2|\n+---------------+-----+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["This is the end of the official test."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9fa271ef-5a0e-47f7-b717-ef641e688120"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2019 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7cdc6c4f-2425-4c2b-a991-6b4c42172d77"}}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.9","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"cse-take-home-assignment-solution","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3741940517731844}},"nbformat":4,"nbformat_minor":0}

{"cells":[{"cell_type":"code","source":["# http://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations\n# http://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis\n\n# Sample Code\n# https://supergloo.com/spark-python/apache-spark-transformations-python-examples\n# https://www.analyticsvidhya.com/blog/2016/10/using-pyspark-to-perform-transformations-and-actions-on-rdd/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Pyspark Transformation","showTitle":true,"inputWidgets":{},"nuid":"9197f7a2-1b1e-4ba3-b47b-9dd5c71bf186"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["## map Transformation\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html#pyspark.RDD.map\n\ntext=[\"you are my sunshine\",\"my only sunshine\"]\ntext_file = sc.parallelize(text)\n# map each line in text to a list of words. 2d array output.\nprint('map:',text_file.map(lambda line: line.split(\" \")).collect())\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83d5f589-3b6d-4f41-b007-25630d261839"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">map: [[&#39;you&#39;, &#39;are&#39;, &#39;my&#39;, &#39;sunshine&#39;], [&#39;my&#39;, &#39;only&#39;, &#39;sunshine&#39;]]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">map: [[&#39;you&#39;, &#39;are&#39;, &#39;my&#39;, &#39;sunshine&#39;], [&#39;my&#39;, &#39;only&#39;, &#39;sunshine&#39;]]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["## filter Transformation\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.flatMap.html#pyspark.RDD.flatMap\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.filter.html#pyspark.RDD.filter\n## Upload the file. \n## https://docs.databricks.com/data/data.html\n\n## Incase DBFS Upload button is not enabled then Go To Admin Console > Workspace Settings and Enable the DBFS.\n## https://docs.databricks.com/administration-guide/admin-console.html\n## https://docs.databricks.com/administration-guide/workspace/dbfs-ui-upload.html\n\nnames = sc.textFile(\"dbfs:/FileStore/data/names.csv\")\nrows = names.map(lambda line: line.split(\",\"))\n\n## Filter rows having name MICHAEL. \nrows.filter(lambda line: \"MICHAEL\" in line).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e565b4e3-b45b-4512-af69-ded22b7430bd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[50]: [[&#39;2013&#39;, &#39;MICHAEL&#39;, &#39;UK&#39;, &#39;M&#39;, &#39;25&#39;], [&#39;2013&#39;, &#39;MICHAEL&#39;, &#39;USA&#39;, &#39;M&#39;, &#39;18&#39;]]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[50]: [[&#39;2013&#39;, &#39;MICHAEL&#39;, &#39;UK&#39;, &#39;M&#39;, &#39;25&#39;], [&#39;2013&#39;, &#39;MICHAEL&#39;, &#39;USA&#39;, &#39;M&#39;, &#39;18&#39;]]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["##  flatMap Transformation\ntext=[\"you are my sunshine\",\"my only sunshine\"]\ntext_file = sc.parallelize(text)\n# create a single list of words by combining the words from all of the lines. 1d array output\nprint('flatmap:',text_file.flatMap(lambda line: line.split(\" \")).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c199095c-b7ca-4ae2-b150-75c4108720e7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">flatmap: [&#39;you&#39;, &#39;are&#39;, &#39;my&#39;, &#39;sunshine&#39;, &#39;my&#39;, &#39;only&#39;, &#39;sunshine&#39;]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">flatmap: [&#39;you&#39;, &#39;are&#39;, &#39;my&#39;, &#39;sunshine&#39;, &#39;my&#39;, &#39;only&#39;, &#39;sunshine&#39;]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["## mapPartitions Transformation\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.mapPartitions.html#pyspark.RDD.mapPartitions\none_to_nine = range(1,10)\nparallel = sc.parallelize(one_to_nine, 3)\ndef f(iterator): yield sum(iterator)\n  \nprint(' Parallelism overidden with partitions ' , parallel.getNumPartitions() )  \nprint(parallel.mapPartitions(f).collect())\n\n#Partition 1: 1+2+3 = 6\n#Partition 2: 4+5+6 = 15\n#Partition 3: 7+8+9 = 24\n\n\n\nparallel = sc.parallelize(one_to_nine)\n\nprint(' Default Parallelism with partitions ' , sc.defaultParallelism)\nprint(' Default Parallelism with partitions ' , parallel.getNumPartitions() )\n\nprint(parallel.mapPartitions(f).collect())\n\n## Explanation of the output\n#Partition 1 = 1\n#Partition 2= 2\n#Partition 3 = 3\n#Partition 4 = 4\n#Partition 5 = 5\n#Partition 6 = 6\n#Partition 7 = 7\n#Partition 8: 8+9 = 17"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"993e6d4d-e66e-40cc-8d02-708db1811e2d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"> Parallelism overidden with partitions  3\n[6, 15, 24]\n Default Parallelism with partitions  8\n Default Parallelism with partitions  8\n[1, 2, 3, 4, 5, 6, 7, 17]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"> Parallelism overidden with partitions  3\n[6, 15, 24]\n Default Parallelism with partitions  8\n Default Parallelism with partitions  8\n[1, 2, 3, 4, 5, 6, 7, 17]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["## mapParititonsWithIndex Transformation\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.mapPartitionsWithIndex.html#pyspark.RDD.mapPartitionsWithIndex\nparallel = sc.parallelize(range(1,10),4)\ndef show(index, iterator): yield 'index: '+str(index)+\" values: \"+ str(list(iterator))\nprint(parallel.mapPartitionsWithIndex(show).collect())\n\n## Changing partition number to 3.\nparallel = sc.parallelize(range(1,10),3)\nprint(parallel.mapPartitionsWithIndex(show).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"400cb841-be3b-4179-8a3a-458900d548af"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[&#39;index: 0 values: [1, 2]&#39;, &#39;index: 1 values: [3, 4]&#39;, &#39;index: 2 values: [5, 6]&#39;, &#39;index: 3 values: [7, 8, 9]&#39;]\n[&#39;index: 0 values: [1, 2, 3]&#39;, &#39;index: 1 values: [4, 5, 6]&#39;, &#39;index: 2 values: [7, 8, 9]&#39;]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;index: 0 values: [1, 2]&#39;, &#39;index: 1 values: [3, 4]&#39;, &#39;index: 2 values: [5, 6]&#39;, &#39;index: 3 values: [7, 8, 9]&#39;]\n[&#39;index: 0 values: [1, 2, 3]&#39;, &#39;index: 1 values: [4, 5, 6]&#39;, &#39;index: 2 values: [7, 8, 9]&#39;]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["## sample Transformation\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sample.html#pyspark.RDD.sample\nparallel = sc.parallelize(range(1,10))\n\n## Without seed - Output could be different or same its not guranteed\nprint(parallel.sample(True,.2).count())\nprint(parallel.sample(True,.2).count())\n\n## With seed - Output will be always same its guranteed.\nprint(parallel.sample(True,.2,seed=1).count())\nprint(parallel.sample(True,.2,seed=1).count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72cce47c-acf8-4ceb-809a-897843325bb2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">0\n2\n3\n3\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0\n2\n3\n3\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["## union Transformation\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.union.html#pyspark.RDD.union\n\nrdd1 = sc.parallelize([1, 1, 2, 3])\nrdd2=sc.parallelize(['a','b',1])\nprint('rdd1=',rdd1.collect())\nprint('rdd2=',rdd2.collect())\nprint('union as bags =',rdd1.union(rdd2).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f097c634-22d2-4430-9f3d-6ddc27cc6ae1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">rdd1= [1, 1, 2, 3]\nrdd2= [&#39;a&#39;, &#39;b&#39;, 1]\nunion as bags = [1, 1, 2, 3, &#39;a&#39;, &#39;b&#39;, 1]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">rdd1= [1, 1, 2, 3]\nrdd2= [&#39;a&#39;, &#39;b&#39;, 1]\nunion as bags = [1, 1, 2, 3, &#39;a&#39;, &#39;b&#39;, 1]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["## intersection Transformation\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.intersection.html#pyspark.RDD.intersection\n\nrdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\nrdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n\nprint('rdd1=',rdd1.collect())\nprint('rdd2=',rdd2.collect())\nprint('intersection =',rdd1.intersection(rdd2).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6c74c12-ce8d-429b-9bab-9a26b9f5c06e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">rdd1= [1, 10, 2, 3, 4, 5]\nrdd2= [1, 6, 2, 3, 7, 8]\nintersection = [1, 2, 3]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">rdd1= [1, 10, 2, 3, 4, 5]\nrdd2= [1, 6, 2, 3, 7, 8]\nintersection = [1, 2, 3]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["## distinct transformation\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.distinct.html#pyspark.RDD.distinct\n\nrdd = sc.parallelize([1,1,2,2,3,4,5,5])\n## Get the distinct values\nprint('Distinct = ' , rdd.distinct().collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f1bc02e-bbbd-4386-9f0b-6ef1df60cf3f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Distinct =  [1, 2, 3, 4, 5]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Distinct =  [1, 2, 3, 4, 5]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["## groupbyKey Transformation\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.groupByKey.html#pyspark.RDD.groupByKey\n\nrdd = sc.parallelize([('a',2), ('b',4), ('b',6)])\nprint(\"Original RDD :\", rdd.collect())\n#print(\"After transformation : \", rdd.groupByKey().mapValues(lambda x:[a for a in x]).collect())\nprint(\"After transformation : \", rdd.groupByKey().mapValues(lambda x:[a+a for a in x]).collect())\n\nprint(\"After transformation : \", rdd.groupByKey().mapValues(len).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9afac5e-3080-49ed-abb7-1d04b815b5c1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Original RDD : [(&#39;a&#39;, 2), (&#39;b&#39;, 4), (&#39;b&#39;, 6)]\nAfter transformation :  [(&#39;a&#39;, [4]), (&#39;b&#39;, [8, 12])]\nAfter transformation :  [(&#39;a&#39;, 1), (&#39;b&#39;, 2)]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Original RDD : [(&#39;a&#39;, 2), (&#39;b&#39;, 4), (&#39;b&#39;, 6)]\nAfter transformation :  [(&#39;a&#39;, [4]), (&#39;b&#39;, [8, 12])]\nAfter transformation :  [(&#39;a&#39;, 1), (&#39;b&#39;, 2)]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["## reduceByKey Transformation\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduceByKey.html#pyspark.RDD.reduceByKey\n\nrdd = sc.parallelize([(1,2), (2,4), (2,6)])\nprint(\"Original RDD :\", rdd.collect())\nprint(\"After transformation : \", rdd.reduceByKey(lambda a,b: a+b).collect())\n\nfrom operator import add\nrdd2 = sc.parallelize([(\"a\", 4), (\"b\", 2), (\"a\", 6)])\nprint(\"Original RDD :\", rdd2.collect())\nprint('After transformation ' , rdd2.reduceByKey(add).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d958ab12-02f2-49a2-9a23-75aebeea59d8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Original RDD : [(1, 2), (2, 4), (2, 6)]\nAfter transformation :  [(1, 2), (2, 10)]\nOriginal RDD : [(&#39;a&#39;, 4), (&#39;b&#39;, 2), (&#39;a&#39;, 6)]\nAfter transformation  [(&#39;a&#39;, 10), (&#39;b&#39;, 2)]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Original RDD : [(1, 2), (2, 4), (2, 6)]\nAfter transformation :  [(1, 2), (2, 10)]\nOriginal RDD : [(&#39;a&#39;, 4), (&#39;b&#39;, 2), (&#39;a&#39;, 6)]\nAfter transformation  [(&#39;a&#39;, 10), (&#39;b&#39;, 2)]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["## aggregateByKey Transformation\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.aggregateByKey.html#pyspark.RDD.aggregateByKey\nnames = sc.textFile(\"dbfs:/FileStore/data/names.csv\")\nprint('Original file ', names.collect())\n\n## Skipping the header row.\nfiltered_rows = names.filter(lambda line: \"Count\" not in line).map(lambda line: line.split(\",\"))\nprint('Filtered rows ' , filtered_rows.collect())\n\n## By Name and Adding the Count. \n## Note MICHAEL is repeated twice hence Count has been sum i.e. 25 + 18.\n## Index of the data : Year - 0, Name - 1, Country - 2, Gender - 3, Count - 4 \nprint('After transformation ' , filtered_rows.map(lambda n:  (str(n[1]), int(n[4]) ) ).aggregateByKey(0, lambda k,v: int(v)+k, lambda v,k: k+v).collect())\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30201d7f-20d8-453c-9827-5d18b483973e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Original file  [&#39;Year,Name,Country,Gender,Count&#39;, &#39;2012,RUSSELL,USA,M,6&#39;, &#39;2012,KATY,UK,F,14&#39;, &#39;2013,JOHN,UK,M,20 &#39;, &#39;2013,MICHAEL,UK,M,25&#39;, &#39;2013,MICHAEL,USA,M,18&#39;, &#39;2013,LINDA,USA,F,20&#39;, &#39;2013,RALPH,USA,M,19&#39;, &#39;2012,ALICE,UK,F,15&#39;]\nFiltered rows  [[&#39;2012&#39;, &#39;RUSSELL&#39;, &#39;USA&#39;, &#39;M&#39;, &#39;6&#39;], [&#39;2012&#39;, &#39;KATY&#39;, &#39;UK&#39;, &#39;F&#39;, &#39;14&#39;], [&#39;2013&#39;, &#39;JOHN&#39;, &#39;UK&#39;, &#39;M&#39;, &#39;20 &#39;], [&#39;2013&#39;, &#39;MICHAEL&#39;, &#39;UK&#39;, &#39;M&#39;, &#39;25&#39;], [&#39;2013&#39;, &#39;MICHAEL&#39;, &#39;USA&#39;, &#39;M&#39;, &#39;18&#39;], [&#39;2013&#39;, &#39;LINDA&#39;, &#39;USA&#39;, &#39;F&#39;, &#39;20&#39;], [&#39;2013&#39;, &#39;RALPH&#39;, &#39;USA&#39;, &#39;M&#39;, &#39;19&#39;], [&#39;2012&#39;, &#39;ALICE&#39;, &#39;UK&#39;, &#39;F&#39;, &#39;15&#39;]]\nAfter transformation  [(&#39;KATY&#39;, 14), (&#39;MICHAEL&#39;, 43), (&#39;LINDA&#39;, 20), (&#39;RALPH&#39;, 19), (&#39;RUSSELL&#39;, 6), (&#39;JOHN&#39;, 20), (&#39;ALICE&#39;, 15)]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Original file  [&#39;Year,Name,Country,Gender,Count&#39;, &#39;2012,RUSSELL,USA,M,6&#39;, &#39;2012,KATY,UK,F,14&#39;, &#39;2013,JOHN,UK,M,20 &#39;, &#39;2013,MICHAEL,UK,M,25&#39;, &#39;2013,MICHAEL,USA,M,18&#39;, &#39;2013,LINDA,USA,F,20&#39;, &#39;2013,RALPH,USA,M,19&#39;, &#39;2012,ALICE,UK,F,15&#39;]\nFiltered rows  [[&#39;2012&#39;, &#39;RUSSELL&#39;, &#39;USA&#39;, &#39;M&#39;, &#39;6&#39;], [&#39;2012&#39;, &#39;KATY&#39;, &#39;UK&#39;, &#39;F&#39;, &#39;14&#39;], [&#39;2013&#39;, &#39;JOHN&#39;, &#39;UK&#39;, &#39;M&#39;, &#39;20 &#39;], [&#39;2013&#39;, &#39;MICHAEL&#39;, &#39;UK&#39;, &#39;M&#39;, &#39;25&#39;], [&#39;2013&#39;, &#39;MICHAEL&#39;, &#39;USA&#39;, &#39;M&#39;, &#39;18&#39;], [&#39;2013&#39;, &#39;LINDA&#39;, &#39;USA&#39;, &#39;F&#39;, &#39;20&#39;], [&#39;2013&#39;, &#39;RALPH&#39;, &#39;USA&#39;, &#39;M&#39;, &#39;19&#39;], [&#39;2012&#39;, &#39;ALICE&#39;, &#39;UK&#39;, &#39;F&#39;, &#39;15&#39;]]\nAfter transformation  [(&#39;KATY&#39;, 14), (&#39;MICHAEL&#39;, 43), (&#39;LINDA&#39;, 20), (&#39;RALPH&#39;, 19), (&#39;RUSSELL&#39;, 6), (&#39;JOHN&#39;, 20), (&#39;ALICE&#39;, 15)]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["## sortByKey Transformation\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sortByKey.html#pyspark.RDD.sortByKey\n\nlist_tupple_pair = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n\nprint(' Transformation sort by first tupple value and print the first element ' , sc.parallelize(list_tupple_pair).sortByKey().first())\n\n\nprint(' Transformation sort by ascending, number partitions = 1 ', sc.parallelize(list_tupple_pair).sortByKey(True, 1).collect())\n\nprint(' Transformation sort by ascending, number partitions = 2 ', sc.parallelize(list_tupple_pair).sortByKey(True, 2).collect() )\n\nlist_1 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\nlist_1.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\nprint(' Sort using custom function ', sc.parallelize(list_1).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d8566ce-6144-49dc-9546-044aa08c93db"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"> Transformation sort by first tupple value and print the first element  (&#39;1&#39;, 3)\n Transformation sort by ascending, number partitions = 1  [(&#39;1&#39;, 3), (&#39;2&#39;, 5), (&#39;a&#39;, 1), (&#39;b&#39;, 2), (&#39;d&#39;, 4)]\n Transformation sort by ascending, number partitions = 2  [(&#39;1&#39;, 3), (&#39;2&#39;, 5), (&#39;a&#39;, 1), (&#39;b&#39;, 2), (&#39;d&#39;, 4)]\n Sort using custom function  [(&#39;a&#39;, 3), (&#39;fleece&#39;, 7), (&#39;had&#39;, 2), (&#39;lamb&#39;, 5), (&#39;little&#39;, 4), (&#39;Mary&#39;, 1), (&#39;was&#39;, 8), (&#39;white&#39;, 9), (&#39;whose&#39;, 6)]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"> Transformation sort by first tupple value and print the first element  (&#39;1&#39;, 3)\n Transformation sort by ascending, number partitions = 1  [(&#39;1&#39;, 3), (&#39;2&#39;, 5), (&#39;a&#39;, 1), (&#39;b&#39;, 2), (&#39;d&#39;, 4)]\n Transformation sort by ascending, number partitions = 2  [(&#39;1&#39;, 3), (&#39;2&#39;, 5), (&#39;a&#39;, 1), (&#39;b&#39;, 2), (&#39;d&#39;, 4)]\n Sort using custom function  [(&#39;a&#39;, 3), (&#39;fleece&#39;, 7), (&#39;had&#39;, 2), (&#39;lamb&#39;, 5), (&#39;little&#39;, 4), (&#39;Mary&#39;, 1), (&#39;was&#39;, 8), (&#39;white&#39;, 9), (&#39;whose&#39;, 6)]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["## join Transformation\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.join.html#pyspark.RDD.join\n\nx_rdd = sc.parallelize([(\"a\", 1), (\"b\", 4)])\ny_rdd = sc.parallelize([(\"a\", 2), (\"a\", 3)])\nprint('x_rdd joined with y_rdd ', x_rdd.join(y_rdd).collect())\n\n\nx_rdd2 = sc.parallelize([(\"a\", 4), (\"b\", 4)])\ny_rdd2 = sc.parallelize([(\"a\", 2), (\"a\", 5), (\"c\", 10) ])\nprint('x_rdd2 joined with y_rdd2 ', x_rdd2.join(y_rdd2).collect())\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6860af63-6802-4675-ad66-719e55833e71"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">x_rdd joined with y_rdd  [(&#39;a&#39;, (1, 2)), (&#39;a&#39;, (1, 3))]\nx_rdd2 joined with y_rdd2  [(&#39;a&#39;, (4, 2)), (&#39;a&#39;, (4, 5))]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">x_rdd joined with y_rdd  [(&#39;a&#39;, (1, 2)), (&#39;a&#39;, (1, 3))]\nx_rdd2 joined with y_rdd2  [(&#39;a&#39;, (4, 2)), (&#39;a&#39;, (4, 5))]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["## \n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.cogroup.html#pyspark.RDD.cogroup\n\nx = sc.parallelize([(\"a\", 1), (\"b\", 4)])\ny = sc.parallelize([(\"a\", 2)])\n\nco_group = x.cogroup(y).collect()\nprint('Co group ' , co_group )\n\nprint('Sample output ', [(x, tuple(map(list, y))) for x, y in sorted(list(co_group))])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4cfc7947-0097-4dd0-ad09-95227d31d1a0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Co group  [(&#39;b&#39;, (&lt;pyspark.resultiterable.ResultIterable object at 0x7fb4f93817f0&gt;, &lt;pyspark.resultiterable.ResultIterable object at 0x7fb4f938db20&gt;)), (&#39;a&#39;, (&lt;pyspark.resultiterable.ResultIterable object at 0x7fb4f938dc10&gt;, &lt;pyspark.resultiterable.ResultIterable object at 0x7fb4f938d310&gt;))]\nSample output  [(&#39;a&#39;, ([1], [2])), (&#39;b&#39;, ([4], []))]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Co group  [(&#39;b&#39;, (&lt;pyspark.resultiterable.ResultIterable object at 0x7fb4f93817f0&gt;, &lt;pyspark.resultiterable.ResultIterable object at 0x7fb4f938db20&gt;)), (&#39;a&#39;, (&lt;pyspark.resultiterable.ResultIterable object at 0x7fb4f938dc10&gt;, &lt;pyspark.resultiterable.ResultIterable object at 0x7fb4f938d310&gt;))]\nSample output  [(&#39;a&#39;, ([1], [2])), (&#39;b&#39;, ([4], []))]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["## cartesian Transformation\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.cartesian.html#pyspark.RDD.cartesian\n\nrdd = sc.parallelize([1, 2])\nprint('Original ', rdd.collect())\nprint('After Cartesian Transformation with self ', rdd.cartesian(rdd).collect())\n\n\nrdd = sc.parallelize(['a', 'b'])\nprint('Original ', rdd.collect())\nprint('After Cartesian Transformation with self ', rdd.cartesian(rdd).collect())\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea8a8783-5fd1-4bb9-8c30-6f01772fe4e9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Original  [1, 2]\nAfter Cartesian Transformation with self  [(1, 1), (1, 2), (2, 1), (2, 2)]\nOriginal  [&#39;a&#39;, &#39;b&#39;]\nAfter Cartesian Transformation with self  [(&#39;a&#39;, &#39;a&#39;), (&#39;a&#39;, &#39;b&#39;), (&#39;b&#39;, &#39;a&#39;), (&#39;b&#39;, &#39;b&#39;)]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Original  [1, 2]\nAfter Cartesian Transformation with self  [(1, 1), (1, 2), (2, 1), (2, 2)]\nOriginal  [&#39;a&#39;, &#39;b&#39;]\nAfter Cartesian Transformation with self  [(&#39;a&#39;, &#39;a&#39;), (&#39;a&#39;, &#39;b&#39;), (&#39;b&#39;, &#39;a&#39;), (&#39;b&#39;, &#39;b&#39;)]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["## pipe Transformation\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.pipe.html#pyspark.RDD.pipe\nprint( ' Pipe transformation using linux command cat ' , sc.parallelize(['1', '2', '', '3']).pipe('cat').collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"494648b1-8ed2-4d1c-b809-177a806664b4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"> Pipe transformation using linux command cat  [&#39;1&#39;, &#39;2&#39;, &#39;&#39;, &#39;3&#39;]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"> Pipe transformation using linux command cat  [&#39;1&#39;, &#39;2&#39;, &#39;&#39;, &#39;3&#39;]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["## coalesce Transformation\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.coalesce.html#pyspark.RDD.coalesce\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.glom.html#pyspark.RDD.glom\n\nprint( ' Original RDD ' , sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect())\nprint( ' Coalesce RDD ' , sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d64a663-b1e0-4a70-8c92-9073c1be6c48"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"> Original RDD  [[1], [2, 3], [4, 5]]\n Coalesce RDD  [[1, 2, 3, 4, 5]]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"> Original RDD  [[1], [2, 3], [4, 5]]\n Coalesce RDD  [[1, 2, 3, 4, 5]]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["## repartition Transformation\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.repartition.html#pyspark.RDD.repartition\nrdd = sc.parallelize([1,2,3,4,5,6,7], 4)\nprint(' RDD with partition ' , rdd.glom().collect())\nprint(' Repartition ' , rdd.repartition(2).glom().collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0417ad0-b796-4f75-a706-346ba901d763"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"> RDD with partition  [[1], [2, 3], [4, 5], [6, 7]]\n Repartition  [[1, 4, 5, 6, 7], [2, 3]]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"> RDD with partition  [[1], [2, 3], [4, 5], [6, 7]]\n Repartition  [[1, 4, 5, 6, 7], [2, 3]]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["## repartitionAndSortWithinPartitions Transformation\n## http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.repartitionAndSortWithinPartitions.html#pyspark.RDD.repartitionAndSortWithinPartitions\nrdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\nprint('rdd ', rdd.glom().collect())\n\nrdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\nprint('rdd2 ', rdd2.glom().collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6da2dd96-edac-4572-a96d-7197a1f49975"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">rdd  [[], [(0, 5)], [(3, 8)], [(2, 6)], [], [(0, 8)], [(3, 8)], [(1, 3)]]\nrdd2  [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">rdd  [[], [(0, 5)], [(3, 8)], [(2, 6)], [], [(0, 8)], [(3, 8)], [(1, 3)]]\nrdd2  [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark_RDD_Transformation_Assignment","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3569580824809396}},"nbformat":4,"nbformat_minor":0}
